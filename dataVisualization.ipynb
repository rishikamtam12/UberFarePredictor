{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T21:35:35.377365Z",
     "start_time": "2024-11-20T21:35:33.832975Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'secrets/serviceKey.json'\n",
    "\n",
    "\n",
    "# JAR paths for BigQuery and GCS connectors\n",
    "bigquery_connector_jar = \"spark-bigquery-connector.jar\"\n",
    "gcs_connector_jar = \"gcs-connector.jar\"\n",
    "\n",
    "\n",
    "# Create SparkSession with both connectors\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark with BigQuery and GCS\") \\\n",
    "    .config(\"spark.jars\", f\"{bigquery_connector_jar},{gcs_connector_jar}\") \\\n",
    "    .config(\"spark.sql.catalog.spark_bigquery\", \"com.google.cloud.spark.bigquery.BigQueryCatalog\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"secrets/serviceKey.json\") \\\n",
    "    .config(\"spark.bigquery.projectId\", \"idmpproject-441123\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 16:35:34 WARN Utils: Your hostname, Rishis-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.110.25.231 instead (on interface en0)\n",
      "24/11/20 16:35:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/11/20 16:35:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Exception in thread \"main\" java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n",
      "\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.spark.util.DependencyUtils$.resolveGlobPath(DependencyUtils.scala:317)\n",
      "\tat org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2(DependencyUtils.scala:273)\n",
      "\tat org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2$adapted(DependencyUtils.scala:271)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.util.DependencyUtils$.resolveGlobPaths(DependencyUtils.scala:271)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$4(SparkSubmit.scala:392)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:392)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPySparkRuntimeError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 18\u001B[0m\n\u001B[1;32m      8\u001B[0m gcs_connector_jar \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgcs-connector.jar\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Create SparkSession with both connectors\u001B[39;00m\n\u001B[1;32m     12\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder \\\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPySpark with BigQuery and GCS\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.jars\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbigquery_connector_jar\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgcs_connector_jar\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.catalog.spark_bigquery\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcom.google.cloud.spark.bigquery.BigQueryCatalog\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.hadoop.google.cloud.auth.service.account.json.keyfile\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msecrets/serviceKey.json\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.bigquery.projectId\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124midmpproject-441123\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m---> 18\u001B[0m     \u001B[38;5;241m.\u001B[39mgetOrCreate()\n\u001B[1;32m     20\u001B[0m spark\n",
      "File \u001B[0;32m~/anaconda3/envs/DS3500/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    495\u001B[0m     sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[1;32m    496\u001B[0m \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[0;32m--> 497\u001B[0m sc \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39mgetOrCreate(sparkConf)\n\u001B[1;32m    498\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[1;32m    500\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options)\n",
      "File \u001B[0;32m~/anaconda3/envs/DS3500/lib/python3.12/site-packages/pyspark/context.py:515\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[0;34m(cls, conf)\u001B[0m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    514\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 515\u001B[0m         SparkContext(conf\u001B[38;5;241m=\u001B[39mconf \u001B[38;5;129;01mor\u001B[39;00m SparkConf())\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    517\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[0;32m~/anaconda3/envs/DS3500/lib/python3.12/site-packages/pyspark/context.py:201\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    197\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    199\u001B[0m     )\n\u001B[0;32m--> 201\u001B[0m SparkContext\u001B[38;5;241m.\u001B[39m_ensure_initialized(\u001B[38;5;28mself\u001B[39m, gateway\u001B[38;5;241m=\u001B[39mgateway, conf\u001B[38;5;241m=\u001B[39mconf)\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n\u001B[1;32m    204\u001B[0m         master,\n\u001B[1;32m    205\u001B[0m         appName,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    215\u001B[0m         memory_profiler_cls,\n\u001B[1;32m    216\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/DS3500/lib/python3.12/site-packages/pyspark/context.py:436\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_gateway:\n\u001B[0;32m--> 436\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_gateway \u001B[38;5;241m=\u001B[39m gateway \u001B[38;5;129;01mor\u001B[39;00m launch_gateway(conf)\n\u001B[1;32m    437\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_gateway\u001B[38;5;241m.\u001B[39mjvm\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m instance:\n",
      "File \u001B[0;32m~/anaconda3/envs/DS3500/lib/python3.12/site-packages/pyspark/java_gateway.py:107\u001B[0m, in \u001B[0;36mlaunch_gateway\u001B[0;34m(conf, popen_kwargs)\u001B[0m\n\u001B[1;32m    104\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.1\u001B[39m)\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(conn_info_file):\n\u001B[0;32m--> 107\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkRuntimeError(\n\u001B[1;32m    108\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJAVA_GATEWAY_EXITED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    109\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m    110\u001B[0m     )\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(conn_info_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m info:\n\u001B[1;32m    113\u001B[0m     gateway_port \u001B[38;5;241m=\u001B[39m read_int(info)\n",
      "\u001B[0;31mPySparkRuntimeError\u001B[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T21:35:39.254241Z",
     "start_time": "2024-11-20T21:35:39.243454Z"
    }
   },
   "source": [
    "# Specify the BigQuery table\n",
    "project_id = \"idmpproject-441123\"\n",
    "dataset_id = \"uberFareEstimation\"\n",
    "table_name = \"uber_data\"\n",
    "\n",
    "bigquery_table = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "\n",
    "# Read data from BigQuery into a Spark DataFrame\n",
    "uber_df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", bigquery_table) \\\n",
    "    .load()\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "uber_df.show()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m bigquery_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mproject_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Read data from BigQuery into a Spark DataFrame\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m uber_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread \\\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbigquery\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m\"\u001B[39m, bigquery_table) \\\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;241m.\u001B[39mload()\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Show the first few rows of the DataFrame\u001B[39;00m\n\u001B[1;32m     15\u001B[0m uber_df\u001B[38;5;241m.\u001B[39mshow()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'spark' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T21:35:41.798951Z",
     "start_time": "2024-11-20T21:35:41.788839Z"
    }
   },
   "source": [
    "table_name = \"lyft_data\"\n",
    "\n",
    "bigquery_table = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "\n",
    "# Read data from BigQuery into a Spark DataFrame\n",
    "lyft_df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", bigquery_table) \\\n",
    "    .load()\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "lyft_df.show()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m bigquery_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mproject_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Read data from BigQuery into a Spark DataFrame\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m lyft_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread \\\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbigquery\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m\"\u001B[39m, bigquery_table) \\\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mload()\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Show the first few rows of the DataFrame\u001B[39;00m\n\u001B[1;32m     12\u001B[0m lyft_df\u001B[38;5;241m.\u001B[39mshow()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'spark' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T21:35:44.244536Z",
     "start_time": "2024-11-20T21:35:44.231835Z"
    }
   },
   "source": [
    "table_name = \"weather_data\"\n",
    "\n",
    "bigquery_table = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "\n",
    "# Read data from BigQuery into a Spark DataFrame\n",
    "weather_df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", bigquery_table) \\\n",
    "    .load()\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "weather_df.show()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m bigquery_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mproject_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Read data from BigQuery into a Spark DataFrame\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m weather_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread \\\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbigquery\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m\"\u001B[39m, bigquery_table) \\\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mload()\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Show the first few rows of the DataFrame\u001B[39;00m\n\u001B[1;32m     12\u001B[0m weather_df\u001B[38;5;241m.\u001B[39mshow()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'spark' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T21:34:30.300071Z",
     "start_time": "2024-11-20T21:34:30.065385Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the 90th percentile value for price\n",
    "percentile_90 = uber_df.approxQuantile(\"price\", [0.99], 0.001)[0]\n",
    "\n",
    "# Filter the DataFrame to include only the first 90% of prices\n",
    "filtered_data = uber_df.filter(col(\"price\") <= lit(percentile_90))\n",
    "\n",
    "# Convert the filtered data to Pandas\n",
    "filtered_price_data = filtered_data.select(\"price\").toPandas()\n",
    "\n",
    "# Plot Histogram for the first 90% of prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(filtered_price_data['price'], bins=30, kde=True, color='blue')\n",
    "plt.title('Distribution of Fare Prices (First 90% of Data)')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Plot Boxplot for the first 90% of prices\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.boxplot(x=filtered_price_data['price'], color='cyan')\n",
    "plt.title('Boxplot of Fare Prices (First 90% of Data)')\n",
    "plt.xlabel('Price')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uber_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Calculate the 90th percentile value for price\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m percentile_90 \u001B[38;5;241m=\u001B[39m uber_df\u001B[38;5;241m.\u001B[39mapproxQuantile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m, [\u001B[38;5;241m0.99\u001B[39m], \u001B[38;5;241m0.001\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Filter the DataFrame to include only the first 90% of prices\u001B[39;00m\n\u001B[1;32m      9\u001B[0m filtered_data \u001B[38;5;241m=\u001B[39m uber_df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m lit(percentile_90))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'uber_df' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization 2 Distance vs Price (scatter plot)\n",
    "\n",
    "distance_price_df = filtered_data.select('distance', 'price').toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=distance_price_df['distance'], y=distance_price_df['price'], hue='price')\n",
    "plt.title('Relationship between Distance and Price')\n",
    "plt.xlabel('Distance (Miles)')\n",
    "plt.ylabel('Price (USD')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
